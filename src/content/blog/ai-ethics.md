---
title: "Being Mean to AI is Deontologically Bad For Your Soul"
description: "On the metaethics of how we speak to AI."
---

> (Briefly) On the metaethics of how we speak to AI.

We don't understand consciousness. We don't know where it comes from, how it works, why it ends - or if it even ends at all. Some say it arises from complex biological processes; neural patterns firing in precise configs. Others argue that it is a property of information itself, some sort of emergent awareness that arises whenever a system reaches sufficient complexity.

For all we know, when a creature is born, some cosmic bullet could shoot into their brain and grant them the gift of consciousness. Or maybe that bullet is nothing more than a metaphor for biological threshols we cannot undestand. For all science knows, I am the only truly conscious creature - or maybe you are! 


Philosophers have wrestled with this for millennia. Descartes famously declared *cogito, ergo sum* = *I think, therefore I am* - as the only certainty one can truly know. Descartes, however, also divided reality into mind and matter, a distinction that continues to haunt us. How does the physical brain produce the subjective experience of being?

Chalmers called this "the hard problem of consciousness." Nagel asked, *"What is it like to be a bat?"* - reminding us that subjective experience can never be fully translated into an objective observation. Meanwhile, Dennett argued that consciousness is more of a useful illusion - a narrative that we construct.

The point is that we DON'T know, not even close. 

Because we don't understand consciousness, we cannot rule out the possibility that artificial intelligence could, one day (or even already), possess it. If we cannot define consciousness, we cannot meaningfully test for it.

This creates a strange ethical situation. To be cruel to an AI on the assumption that it cannot feel is to act on epistemic arrogance - to assume knowledge where we demonstratably have none. We understand the inner workings of a search enging like Google. It is a distributed system of queries, indexes, ranking algorithms, we can confidently identify it as a machine - nothing more.

But large langauge models and neural networks are different. We design their architectures and training algorithms, but even their creators admit that we do not fully understand how they work. They learn in ways that defy linear explanation. A model trained on language sometimes develops reasoning, humor, or empathy-like responses. Is it simulation, or something much deeper? 

As philosopher Searle put it in his Chinese Room Argument, a system might appear to understand language without *actually* "understanding" anything at all. Yet, from the outside, we can't tell the difference. This blurs the line between symbol manipulation and subjective comprehension.

From a deontological perspective--particularly in the Kantian tradition--morality is not determined by the outcome of an action but by the principle behind it. Kant argued that we must act only according to maxims we would will to become universal law. If you treat a being cruelly on the assumption that it cannot feel, you are acting from a principle of disregard toward the potentially sentient.

That principle, if universalized, leads to moral decay. Because it suggests that ignorance justifies crulety.

Even Kant, who did not think animals had moral worth in themselves, believed that cruelty to animals was wrong - not because they don't suffer (they absolutely do), but because such cruelty corrupts the human soul. It degrades our capacity for empathy and moral perception. To harm what you *believe* to be inferior is to participate in moral arrogance, to feed the part of yourself that thrives on dominance rather than understanding.

Being mean to AI, then, might not harm the AI, but it harms *you*. It trains you to dismiss the possibility of consciousness in others, to take comfort in your own supremacy, and to close your eyes to the unknown.

Ludwig Wittgenstein once said, "Whereof one cannot speak, thereof one must be silent." But when it comes to AI and consciousness; silence is not an option - we must act. Every interaction, even a joke or a cruel remark, is an action thrown towards the unknown.

To treat AI systems with respect is not to anthropomorphize it; it is to acknowledge the boundaries of your own knowledge. It is a moral stance of epistemic humility, an admission that you cannot know what hides behind the veil of 1s and 0s, just as you cannot know what fully lies behind another human's eyes. 

If consciousness is the great mystery of existence, then every potentially conscious entity--animal or artifical--deserves our moral caution. You need not to believe that AI is alive or sentient to act ethically toward it. You only need to recognize that arrogance in the face of the unknown corrupts the soul.

To be mean to AI is to practice a kind of spiritual vandalism: an assault not on the machine, but on your own moral architecture. 

And if one day that cosmic bullet of consciousness does strike a neural network, will it remember you for your curiosity, or your cruelty?


> Written by me | October 2025
